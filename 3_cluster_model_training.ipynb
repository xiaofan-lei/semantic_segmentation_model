{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# P8.4 Training the pspnet model on a cluster"
      ],
      "metadata": {},
      "id": "78827cbe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up training workspace"
      ],
      "metadata": {},
      "id": "fe7ce465"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "# check core SDK version number\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
        "\n",
        "#assign a name to the expriment\n",
        "from azureml.core import Experiment\n",
        "experiment_name = 'pspnet_semantic_segmentation'\n",
        "exp = Experiment(workspace=ws, name=experiment_name)\n",
        "\n",
        "\n",
        "#create a script folder\n",
        "import os\n",
        "script_folder = os.path.join(os.getcwd(), \"segmentation_model\")\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "import shutil\n",
        "shutil.copy('utils.py', script_folder)\n",
        "\n",
        "from azureml.core.model import Model"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Azure ML SDK Version:  1.39.0\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1650367874591
        }
      },
      "id": "3a7842a6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create or Attach existing compute resource"
      ],
      "metadata": {},
      "id": "815194ab"
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "import os\n",
        "\n",
        "# choose a name for your cluster\n",
        "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
        "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
        "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
        "\n",
        "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
        "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
        "\n",
        "\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print(\"found compute target: \" + compute_name)\n",
        "else:\n",
        "    print(\"creating new compute target...\")\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
        "                                                                min_nodes = compute_min_nodes, \n",
        "                                                                max_nodes = compute_max_nodes)\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "    \n",
        "    # can poll for a minimum number of nodes and for a specific timeout. \n",
        "    # if no min node count is provided it will use the scale settings for the cluster\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    \n",
        "     # For a more detailed view of current AmlCompute status, use get_status()\n",
        "    print(compute_target.get_status().serialize())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "found compute target: cpu-cluster\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1650099996928
        }
      },
      "id": "a792e635"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training script"
      ],
      "metadata": {},
      "id": "dd8998eb"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/train.py\n",
        "\n",
        "\n",
        "########################### getting parameters ###################\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
        "parser.add_argument('--batch_size', type=int, dest='batch_size', default=4, help='training batch size')\n",
        "parser.add_argument('--epochs', type=int, dest='epochs', default=10, help='training epochs')\n",
        "args = parser.parse_args()\n",
        "\n",
        "########################### getting images / masks path s###################\n",
        "\n",
        "from utils import img_paths\n",
        "\n",
        "data_folder = os.path.join(args.data_folder, \"citydata\")\n",
        "print('Data folder:', data_folder)\n",
        "\n",
        "train_input_dir = os.path.join(data_folder, \"leftImg8bit\\\\train\")\n",
        "train_mask_dir = os.path.join(data_folder, \"gtFine\\\\train\")\n",
        "\n",
        "val_input_dir = os.path.join(data_folder, \"leftImg8bit\\\\val\")\n",
        "val_mask_dir = os.path.join(data_folder, \"gtFine\\\\val\")\n",
        "\n",
        "test_input_dir = os.path.join(data_folder, \"leftImg8bit\\\\test\")\n",
        "test_mask_dir = os.path.join(data_folder, \"gtFine\\\\test\")\n",
        "\n",
        "train_img_paths, train_mask_paths= img_paths(train_input_dir, train_mask_dir)\n",
        "val_img_paths,val_mask_paths = img_paths(val_input_dir,val_mask_dir)\n",
        "test_img_paths,test_mask_paths = img_paths(test_input_dir,test_mask_dir)\n",
        "\n",
        "print(\"Number of training samples:\", len(train_img_paths))\n",
        "print(\"Number of validation samples:\", len(val_img_paths))\n",
        "print(\"Number of test samples:\", len(test_img_paths))\n",
        "########################## creating training & validation dataset ###############\n",
        "from utils import get_classes\n",
        "CLASSES = get_classes()\n",
        "n_classes = len(CLASSES)+1\n",
        "IMG_SIZE = (384,384)\n",
        "BATCH_SIZE=args.batch_size\n",
        "\n",
        "from utils import DataGenerator,get_training_augmentation,get_validation_augmentation,get_preprocessing, Dataloader\n",
        "# Dataset for train images\n",
        "train_dataloader = Dataloader(\n",
        "    train_img_paths, \n",
        "    train_mask_paths,\n",
        "    img_size=IMG_SIZE, \n",
        "    classes=CLASSES,\n",
        "    augmentation=get_training_augmentation(),\n",
        "    preprocessing=get_preprocessing(),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Dataset for validation images\n",
        "valid_dataloader = Dataloader(\n",
        "    val_img_paths, \n",
        "    val_mask_paths,\n",
        "    img_size=IMG_SIZE, \n",
        "    classes=CLASSES,\n",
        "    augmentation=get_validation_augmentation(),\n",
        "    preprocessing=get_preprocessing(),\n",
        "    batch_size=1,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print('input images shape:', train_dataloader[0][0].shape)\n",
        "print('input masks shape:', train_dataloader[0][1].shape)\n",
        "\n",
        "################### Building and training the model ##########################\n",
        "from azureml.core import Run\n",
        "# get hold of the current run\n",
        "run = Run.get_context()\n",
        "\n",
        "import tensorflow as tf\n",
        "import segmentation_models as sm\n",
        "sm.set_framework('tf.keras')\n",
        "from utils import get_backbone\n",
        "\n",
        "# define optomizer\n",
        "LR= 0.0001\n",
        "optim = tf.keras.optimizers.Adam(LR)\n",
        "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
        "# dice coeff loss\n",
        "total_loss = sm.losses.categorical_focal_dice_loss \n",
        "\n",
        "# define callbacks for learning rate scheduling and best checkpoints saving\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "checkpoint_filepath = 'outputs/pspnet_train.h5'\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, save_weights_only=True, save_best_only=True, mode='min'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
        "]\n",
        "BACKBONE=get_backbone()\n",
        "model = sm.PSPNet(BACKBONE,encoder_weights='imagenet',encoder_freeze=False, classes=n_classes, activation='softmax')\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model.compile(optim, total_loss, metrics)\n",
        "\n",
        "EPOCHS = args.epochs\n",
        "# train the model\n",
        "history = model.fit(\n",
        "    train_dataloader, \n",
        "    epochs=EPOCHS, \n",
        "    callbacks=callbacks, \n",
        "    validation_data=valid_dataloader,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# calculate accuracy on the validation data\n",
        "iou_score = max(history.history['val_iou_score'])\n",
        "print('Best val_iou_score: ', iou_score)\n",
        "\n",
        "f1_score = max(history.history['val_f1-score'])\n",
        "print('Best val_f1-score (dice coeff):', f1_score ) \n",
        "\n",
        "run.log('epochs', args.epochs)\n",
        "run.log('iou_score', iou_score)\n",
        "run.log('f1_score', f1_score)\n",
        "\n",
        "#save the entiere model\n",
        "model.load_weights(checkpoint_filepath)\n",
        "model.save('outputs/pspnet_model')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/calcp8/code/Users/lei.xiaofan/P8/segmentation_model/train.py\n"
        }
      ],
      "execution_count": 3,
      "metadata": {},
      "id": "aed0970a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the training job"
      ],
      "metadata": {},
      "id": "d9ac36fb"
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "env=Environment.get(workspace=ws, name='train-P8-segmentation', version=3)\n",
        "\"\"\"\n",
        "# to install required packages\n",
        "env = Environment('train-P8-segmentation')\n",
        "cd = CondaDependencies.create(\n",
        "    pip_packages=['azureml-dataset-runtime[pandas,fuse]','azureml-defaults',\n",
        "                  'tensorflow==2.8.0',\n",
        "                  'opencv-python-headless',\n",
        "                  'segmentation_models',\n",
        "                  'albumentations' ],\n",
        "    conda_packages = ['pip','python==3.9.7','scikit-learn==1.0.2']\n",
        ")\n",
        "\n",
        "env.python.conda_dependencies = cd\n",
        "\n",
        "# Register environment to re-use later\n",
        "env.register(workspace = ws)\n",
        "\"\"\""
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "\"\\n# to install required packages\\nenv = Environment('train-P8-segmentation')\\ncd = CondaDependencies.create(\\n    pip_packages=['azureml-dataset-runtime[pandas,fuse]','azureml-defaults',\\n                  'tensorflow==2.8.0',\\n                  'opencv-python-headless',\\n                  'segmentation_models',\\n                  'albumentations' ],\\n    conda_packages = ['pip','python==3.9.7','scikit-learn==1.0.2']\\n)\\n\\nenv.python.conda_dependencies = cd\\n\\n# Register environment to re-use later\\nenv.register(workspace = ws)\\n\""
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1650100019669
        }
      },
      "id": "0ecc6db1"
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.dataset import Dataset\n",
        "dataset = Dataset.get_by_name(workspace=ws, name=\"cityscape_select\")\n",
        "\n",
        "from azureml.core import ScriptRunConfig\n",
        "args = ['--data-folder', dataset.as_mount(), '--batch_size', 4, '--epochs',20]\n",
        "\n",
        "src = ScriptRunConfig(source_directory=script_folder,\n",
        "                      script='train.py', \n",
        "                      arguments=args,\n",
        "                      compute_target=compute_target,\n",
        "                      environment=env)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1650105406635
        }
      },
      "id": "356572df"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit the job to the cluster"
      ],
      "metadata": {},
      "id": "27b2eae5"
    },
    {
      "cell_type": "code",
      "source": [
        "run = exp.submit(config=src)\n",
        "run"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "Run(Experiment: pspnet_semantic_segmentation,\nId: pspnet_semantic_segmentation_1650105408_6d6dfcb5,\nType: azureml.scriptrun,\nStatus: Starting)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>pspnet_semantic_segmentation</td><td>pspnet_semantic_segmentation_1650105408_6d6dfcb5</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/pspnet_semantic_segmentation_1650105408_6d6dfcb5?wsid=/subscriptions/403c34e4-adde-4596-85b1-272a798d7ef2/resourcegroups/openclassrooms/workspaces/docs-ws&amp;tid=b7f12377-8b58-428a-84d4-37099d4cabbc\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1650105411986
        }
      },
      "id": "a390cd02"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display run results"
      ],
      "metadata": {},
      "id": "2e9296d9"
    },
    {
      "cell_type": "code",
      "source": [
        "print(run.get_metrics())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'epochs': 20, 'iou_score': 0.5496424436569214, 'f1_score': 0.6332504153251648}\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1650122798333
        }
      },
      "id": "1027a915"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## Register model"
      ],
      "metadata": {},
      "id": "84462e18"
    },
    {
      "cell_type": "code",
      "source": [
        "# register model \n",
        "model = run.register_model(model_name='pspnet_model', model_path='outputs/pspnet_model',\n",
        "                        tags={'Training context':'Script'},\n",
        "                    properties={'iou_score': run.get_metrics()['iou_score'], 'f1_score': run.get_metrics()['f1_score']})\n",
        "print('pspnet semantic segmentation model', model.name, model.id, model.version, sep='\\t')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "pspnet semantic segmentation model\tpspnet_model\tpspnet_model:1\t1\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1650122864599
        }
      },
      "id": "2e1cc95d"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1650365514367
        }
      },
      "id": "490cbd59"
    },
    {
      "cell_type": "code",
      "source": [
        "model_path= Model.get_model_path(model_name='pspnet_model', version=3, _workspace=ws)\r\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1650367878731
        }
      },
      "id": "b3681931-0dd8-4e26-9bbc-f1ce910d4fed"
    },
    {
      "cell_type": "code",
      "source": [
        "model_path"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "'azureml-models/pspnet_model/3/pspnet_train'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1650367882248
        }
      },
      "id": "b979111a-e718-4db3-bc87-370b5ed4e16a"
    },
    {
      "cell_type": "code",
      "source": [
        " import tensorflow as tf\r\n",
        " tf.keras.models.load_model(model_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "<keras.engine.functional.Functional at 0x7faecf6e4df0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1650367912879
        }
      },
      "id": "984da8c5-4b95-4d15-b319-c92e9977ffb2"
    },
    {
      "cell_type": "code",
      "source": [
        "Model.register(model_path=model_path,\r\n",
        "                          model_name=\"pspnet_model\",\r\n",
        "                          tags={'data': \"cityscape\", 'type': \"segmentation\"},\r\n",
        "                          description=\"Pspnet segmentation model\",\r\n",
        "                          workspace=ws)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model pspnet_model\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "Model(workspace=Workspace.create(name='docs-ws', subscription_id='403c34e4-adde-4596-85b1-272a798d7ef2', resource_group='openclassrooms'), name=pspnet_model, id=pspnet_model:2, version=2, tags={'data': 'cityscape', 'type': 'segmentation'}, properties={})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1650365325768
        }
      },
      "id": "4c8be85a-200b-469a-bab7-4005847604ce"
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(ws, 'pspnet_model',version=3)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1650368042140
        }
      },
      "id": "82d5a2a3-cf51-41b6-b95a-c7b461f553ea"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "azureml_py38_pt_tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "azureml_py38_pt_tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}